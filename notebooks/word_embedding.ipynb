{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a25e39",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "The data that we start with are hundreds of thousands of comments related to some UK hospitals. Each are associated with a label referring to whether the comment is useful (labeled 1) or not (labeled 0) at indicating an effective treatment. I have already cleaned the comments (e.g. small case, punctuation and stop words removed), but I will not share them since the comment/label associations are from private communication.\n",
    "\n",
    "In this notebook I show how I perform word embedding using the vocabulary defined in the data_prep.ipynb. This process converts the comments into vectors which will be ingested by the NLP algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418a1a9",
   "metadata": {},
   "source": [
    "We first import the vocabulary that we have defined in the data_prep.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2f1e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../vocabulary.txt', 'r') as f:\n",
    "    word_in_vocab = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92989425",
   "metadata": {},
   "source": [
    "We also define the dataframe that contains the cleaned comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d56f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/efftreat_clean_label.csv')\n",
    "comments = df['comments_clean'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c7a0c2",
   "metadata": {},
   "source": [
    "We use the binary bag of words embedding, which means that words in the vocabulary are replaced by ones or zeros depending whether they appear in the comment or not, respectively.\n",
    "\n",
    "Here is an example for the sentence \"jon has a red car\". Let's say our vocabulary has been defined as ['car', 'bird', 'blue', 'red', 'pigeon']. Therefore, our embedding for the sentence results in [1,0,0,1,0], which has the size of our vocabulary, and where ones correspond to words that appear in the comments.\n",
    "\n",
    "Therefore, because we have defined a vocabulary containing 1405 individual words, each of the comments are replaced by sparse vectors of size 1405. We further recall that we have defined a max cut at 50 words for the length of the comments, as explained in the data_prep.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ce2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 114909/114909 [04:09<00:00, 460.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import tqdm\n",
    "\n",
    "max_length = 50\n",
    "embedded_array = []\n",
    "with tqdm(total=len(comments)) as pbar:\n",
    "    for comment_i in comments:\n",
    "        \n",
    "        if len(comment_i.split(' ')) > max_length:\n",
    "            comment_i = ' '.join(comment_i.split(' ')[0:max_length])\n",
    "\n",
    "        embedded_sentence = []\n",
    "        for word_i in word_in_vocab:\n",
    "            if word_i in comment_i.split(' '):\n",
    "                embedded_sentence.append(1)\n",
    "            else:\n",
    "                embedded_sentence.append(0)\n",
    "     \n",
    "        embedded_array.append(embedded_sentence)\n",
    "        pbar.update()\n",
    "\n",
    "embedded_array = np.array(embedded_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a8329e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114909, 1405)\n"
     ]
    }
   ],
   "source": [
    "print(embedded_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794e532",
   "metadata": {},
   "source": [
    "The matrix embedded_array has a shape (number of comments, length of the vocabulary). This embedding is used to pass the comments to the NLP algorithm, presented in the nlp_model.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262570e5",
   "metadata": {},
   "source": [
    "### Concluding remarks\n",
    "\n",
    "Throughout this notebook I show how I transform cleaned comments written in English to a sparse matrix that will be passed to the NLP algorithm. We note that the binary bag of words is not the most efficient way to embed text, since mostly consisting of large sparse vectors populated by ones and zeros, and it does not contain any information about the meaning of the sentence. However, at this early stage it provides one of the simplest way to embed text. Future improvements will include testing various embeddings such as using n-grams or word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d9d460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
